{"block_file": {"data_exporters/export_binance.py:data_exporter:python:export binance": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'dataengineering-411512.cryptopulse.binance'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/export_binance.py", "language": "python", "type": "data_exporter", "uuid": "export_binance"}, "data_exporters/export_fear_greed_index.py:data_exporter:python:export fear greed index": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'dataengineering-411512.cryptopulse.fear_and_greed_index'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/export_fear_greed_index.py", "language": "python", "type": "data_exporter", "uuid": "export_fear_greed_index"}, "data_exporters/export_reddit.py:data_exporter:python:export reddit": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'dataengineering-411512.cryptopulse.reddit'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/export_reddit.py", "language": "python", "type": "data_exporter", "uuid": "export_reddit"}, "data_exporters/unifying_smoke.py:data_exporter:python:unifying smoke": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/unifying_smoke.py", "language": "python", "type": "data_exporter", "uuid": "unifying_smoke"}, "data_loaders/load_fear_greed_index.py:data_loader:python:load fear greed index": {"content": "import io\nimport pandas as pd\nimport json\nimport requests\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef calculate_days_difference(start_date_str, end_date):\n    # Convert the start date string to a datetime object\n    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n    \n    # Calculate the difference between the end date and start date\n    difference = end_date - start_date\n    \n    # Return the difference in days\n    return difference.days\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    limit = calculate_days_difference(kwargs['start_date'], kwargs['execution_date'])\n    parameters = {\n        \"limit\":       limit, # 360 days + 104 days (1 year + 3 months\n        \"format\":      \"json\",\n        \"date_format\":  \"world\"\n    }\n\n    url = 'https://api.alternative.me/fng/'\n    response = requests.get(\n        url    = url,\n        params = parameters\n    )\n    try:\n        response.raise_for_status()\n\n    except requests.exceptions.HTTPError as e:\n        print(f\" -> An error occured: {e}\")\n        return None\n\n    else:\n        print(\" -> Request was successful.\")\n        return pd.DataFrame(response.json()[\"data\"])\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    isinstance(output, pd.DataFrame), 'The output is not a pandas DataFrame'\n", "file_path": "data_loaders/load_fear_greed_index.py", "language": "python", "type": "data_loader", "uuid": "load_fear_greed_index"}, "data_loaders/load_reddit.py:data_loader:python:load reddit": {"content": "import requests\nfrom datetime import datetime, timedelta\nimport time\nfrom typing import List\nimport pandas as pd\nimport numpy as np\nimport praw\nfrom typing import List\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef fetch_subreddit_data(\n    authentication: dict,\n    subreddit_name: str,\n    sort: str,\n    months_back: int,\n    requests_per_minute: int,\n    limit_per_request: int,\n    keywords_list: List[str]\n    ) -> pd.DataFrame:\n    \"\"\"\n    Fetches data from a subreddit based on the given parameters.\n    \"\"\"\n    try:\n        authentication = praw.Reddit(\n            client_id     = authentication['REDDIT_CLIENT_ID'],\n            client_secret = authentication['REDDIT_CLIENT_SECRET'],\n            user_agent    = authentication['REDDIT_USER_AGENT'],\n            username      = authentication['REDDIT_USERNAME'],\n            password      = authentication['REDDIT_PASSWORD']\n        )\n        authentication.user.me()\n    except praw.exceptions.PRAWException as e:  # Adjust the exception type as needed\n        raise ValueError(\"Failed to authenticate with Reddit API.\") from e\n    else:\n        print(\"API connection established successfully.\")\n\n    # Fetch subreddit data\n    subreddit = authentication.subreddit(subreddit_name)\n    end_date = datetime.utcnow() - timedelta(days=30 * months_back)\n    submission_generator = getattr(subreddit, sort)(limit=limit_per_request)\n    posts_dict = {\n        \"ID\": [], \"Author\": [], \"Title\": [],  \"Body\": [], \"Score\": [], \"Total Comments\": [], \"Votes\": [], \"URL\": [], \"Date\": []\n    }\n    request_count = 0\n\n    for submission in submission_generator:\n        submission_date = datetime.utcfromtimestamp(submission.created_utc)\n        if submission_date < end_date:\n            break  # Stop if the post is older than n months\n\n        # Check if the post contains any of the keywords (do not include the post if it does not contain any of the keywords)\n        if not any(\n            keyword.lower() in submission.title.lower()\n            for keyword in keywords_list\n        ):\n            continue\n\n        # Append post data to the dictionary\n        posts_dict[\"ID\"].append(submission.id)\n        posts_dict[\"Author\"].append(submission.author.name if submission.author else 'Deleted')\n        posts_dict[\"Title\"].append(submission.title)\n        posts_dict[\"Body\"].append(submission.selftext if submission.selftext else \"\")\n        posts_dict[\"Score\"].append(submission.score)\n        posts_dict[\"Total Comments\"].append(submission.num_comments)\n        posts_dict[\"Votes\"].append(submission.upvote_ratio)\n        posts_dict[\"URL\"].append(submission.url)\n        posts_dict[\"Date\"].append(submission_date.strftime('%Y-%m-%d'))\n        request_count += 1 # Increment the request count\n        if request_count >= requests_per_minute:\n            time.sleep(60)  # Sleep to adhere to the rate limit\n            request_count = 0  # Reset the request count\n\n    return pd.DataFrame(posts_dict)\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n\n\n    authentication  = {\n        'REDDIT_CLIENT_ID':     get_secret_value('REDDIT_CLIENT_ID'),\n        'REDDIT_CLIENT_SECRET': get_secret_value('REDDIT_CLIENT_SECRET'),\n        'REDDIT_USER_AGENT':    get_secret_value('REDDIT_USER_AGENT'),\n        'REDDIT_USERNAME':      get_secret_value('REDDIT_USERNAME'),\n        'REDDIT_PASSWORD':      get_secret_value('REDDIT_PASSWORD')\n    }#\n    # 2) Define the parameters\n    #------------------------------------------------------\n    subreddit_name      = 'CryptoCurrency'\n    sort                = 'new'\n    months_back         = 6\n    requests_per_minute = 1000  # Be mindful of Reddit's rate limit.\n    limit_per_request   = 1000\n    keywords_list       = ['bitcoin', 'btc']\n   \n    data = fetch_subreddit_data(\n        authentication      = authentication,\n        subreddit_name      = subreddit_name,\n        sort                = sort,\n        months_back         = months_back,\n        requests_per_minute = requests_per_minute,\n        limit_per_request   = limit_per_request,\n        keywords_list       = keywords_list\n    )\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_reddit.py", "language": "python", "type": "data_loader", "uuid": "load_reddit"}, "data_loaders/load_binance.py:data_loader:python:load binance": {"content": "import io\nimport pandas as pd\nimport requests\nfrom datetime import datetime\nfrom binance import Client\nfrom binance.exceptions import BinanceAPIException, BinanceRequestException\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nimport pandas as pd\n\n\n# Importing decorators if they're not already defined\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef fetch_binance_data(\n    api_key: str,\n    secret_key: str,\n    symbol: str,\n    interval: str,\n    start_str: str\n) -> pd.DataFrame:\n    \"\"\"\n    Fetches data from Binance based on the given parameters.\n    \"\"\"\n    # Establish connection to the Binance API\n    client = Client(\n        api_key,\n        secret_key\n    )\n    # try except block to evaluate whether api key and secret key are valid and connection is established\n    try:\n        client.get_account()\n    except (BinanceAPIException, BinanceRequestException) as e:\n        raise ValueError(f\"API Error: {e.status_code} - {e.message}\") from e\n    else:\n        print(\"API connection established successfully.\")\n\n    # Fetch historical klines\n    klines = client.get_historical_klines(\n        symbol    = symbol,\n        interval  = interval,\n        start_str = start_str,\n    )\n    return pd.DataFrame(\n        data    = klines,\n        columns = [\n            \"Open Time\",\n            \"Open\",\n            \"High\",\n            \"Low\",\n            \"Close\",\n            \"Volume\",\n            \"Close Time\",\n            \"Quote Asset Volume\",\n            \"Number of Trades\",\n            \"Taker Buy Base Asset Volume\",\n            \"Taker Buy Quote Asset Volume\",\n            \"Ignore\",\n        ],\n    )\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Load historical daily data for a symbol from Binance API.\n    \"\"\"\n    days_in_past = 360 + 104 # 1 year + 3 months\n    symbol       = kwargs['symbol']\n    interval     = Client.KLINE_INTERVAL_1DAY\n    start_str    = datetime.strptime(kwargs['start_date'], \"%Y-%m-%d\").strftime('%d %b %Y')\n\n    data = fetch_binance_data(\n        api_key    = get_secret_value('bi_key'),\n        secret_key = get_secret_value('bi_secret'),\n        symbol     = symbol,\n        interval   = interval,\n        start_str  = start_str\n    )\n\n    return data\n    \n    \n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test code for testing the output of the data loader.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert not output.empty, 'The output dataframe is empty'\n    assert 'Open' in output.columns, 'DataFrame should contain \"Open\" column'\n", "file_path": "data_loaders/load_binance.py", "language": "python", "type": "data_loader", "uuid": "load_binance"}, "transformers/transform_reddit.py:transformer:python:transform reddit": {"content": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import when, col, year, month\nfrom pyspark.sql import functions as F\n\nimport pandas as pd\n\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    spark = SparkSession.builder.appName(\"RedditDataBatchProcessing\").getOrCreate()\n    df    = spark.createDataFrame(data)\n\n    # Ensure column expressions are correct for string matching\n    df = df.withColumn(\n        \"Currency\",\n        when(\n            (col('Title').rlike('(?i)bitcoin|btc')) | (col('Body').rlike('(?i)bitcoin|btc')), 'Bitcoin'\n        ).when(\n            (col('Title').rlike('(?i)ethereum|eth')) | (col('Body').rlike('(?i)ethereum|eth')), 'Ethereum'\n        ).otherwise('Other')\n    )\n#\n\n    # Deduplicate, sort, and cast types\n    df = df.dropDuplicates(['Title'])\n    df = df.sort(col(\"Date\").asc())\n\n    df = df.select(\n        col(\"ID\").cast(\"string\"),\n        col(\"Author\").cast(\"string\"),\n        col(\"Title\").cast(\"string\"),\n        col(\"Body\").cast(\"string\"),\n        col(\"Score\").cast(\"integer\"),\n        col(\"Total Comments\").cast(\"integer\"),\n        col(\"Votes\").cast(\"float\"),\n        col(\"URL\").cast(\"string\"),\n        col(\"Date\").cast(\"timestamp\"),\n        col(\"Currency\").cast(\"string\")\n    )\n    # create Title + Body column\n    # Assuming 'df' is your DataFrame\n    df = df.withColumn(\n        \"Title_Body\",\n        F.when(F.col(\"Body\").isNull(), F.col(\"Title\"))  # If Body is null, use Title\n        .otherwise(F.concat(F.col(\"Title\"), F.lit(\"\\n\"), F.col(\"Body\")))  # Otherwise, concatenate Title, newline, and Body\n    )\n\n    # Extract year and month from the Open Time column for partitioning\n    df = df.withColumn(\"Year\", year(col(\"Date\")))\n    df = df.withColumn(\"Month\", month(col(\"Date\")))\n\n    return df.toPandas()\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_reddit.py", "language": "python", "type": "transformer", "uuid": "transform_reddit"}, "transformers/transform_fear_greed_index.py:transformer:python:transform fear greed index": {"content": "from pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.functions import col, to_date, year, month, to_timestamp\nimport pandas as pd\n#\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data: pd.DataFrame, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    # Initialize SparkSession\n    spark = SparkSession.builder \\\n        .appName(\"AlternativeMeBatchProcessing\") \\\n        .getOrCreate()\n    # Convert Pandas DataFrame to Spark DataFrame\n    df = spark.createDataFrame(data)\n\n    df = (\n        df.withColumnRenamed(\"timestamp\", \"Date\")\n          #.withColumn(\"Date\", to_date(col(\"Date\"), \"dd-MM-yyyy\"))\n          .withColumn(\"Date\", to_timestamp(col(\"Date\"), \"dd-MM-yyyy\"))  # Casting to timestamp\n          .withColumn(\"value\", col(\"value\").cast(\"integer\"))\n          .withColumn(\"value_classification\", col(\"value_classification\").cast(\"string\"))\n          .drop(\"time_until_update\")\n          .sort(\"Date\")\n    )\n\n    # Add 'Year' and 'Month' columns for partitioning\n    df = (\n        df.withColumn(\"Year\", year(\"Date\"))\n          .withColumn(\"Month\", month(\"Date\"))\n    )\n\n    return df.toPandas()\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    isinstance(output, pd.DataFrame), # check if output is spark dataframe\n", "file_path": "transformers/transform_fear_greed_index.py", "language": "python", "type": "transformer", "uuid": "transform_fear_greed_index"}, "transformers/transform_binance.py:transformer:python:transform binance": {"content": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, from_unixtime, year, month\nimport pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    spark = SparkSession.builder \\\n        .appName(\"AlternativeMeBatchProcessing\") \\\n        .getOrCreate()\n\n    df = spark.createDataFrame(data)\n\n    # Convert timestamps to datetime and cast data types\n    df = df.select(\n        from_unixtime(col(\"Open Time\") / 1000).alias(\"Open Time\").cast(\"timestamp\"), # timestamp before\n        col(\"Open\").cast(\"float\"),\n        col(\"High\").cast(\"float\"),\n        col(\"Low\").cast(\"float\"),\n        col(\"Close\").cast(\"float\"),\n        col(\"Volume\").cast(\"float\"),\n        from_unixtime(col(\"Close Time\") / 1000).alias(\"Close Time\").cast(\"timestamp\"),\n        col(\"Quote Asset Volume\").cast(\"float\"),\n        col(\"Number of Trades\").cast(\"integer\"),\n        col(\"Taker Buy Base Asset Volume\").cast(\"float\"),\n        col(\"Taker Buy Quote Asset Volume\").cast(\"float\")\n    ).drop(\"Ignore\")\n\n    # Extract year and month from the Open Time column for partitioning\n    df = df.withColumn(\"Year\", year(col(\"Open Time\")))\n    df = df.withColumn(\"Month\", month(col(\"Open Time\")))\n\n    \n    return df.toPandas()\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_binance.py", "language": "python", "type": "transformer", "uuid": "transform_binance"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}